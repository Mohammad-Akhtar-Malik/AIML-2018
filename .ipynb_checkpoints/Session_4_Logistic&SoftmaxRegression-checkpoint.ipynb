{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This session is a primer for Softmax / Logistic Regression\n",
    "Logistic regression is a simple classification algorithm for learning to predict binary labels. Softmax Regression is the generalized form of logistic regression. \n",
    "\n",
    "![](./images/logistic_regression_schematic.png)\n",
    "\n",
    "Logistic regression learns weights so as to maximize the likelihood of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Latex\n",
    "from IPython.display import Math\n",
    "import sys\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "\n",
    "![](./images/sigmoid.png)\n",
    "\n",
    "The function is often called the “sigmoid” or “logistic” function – it is an S-shaped function that “squashes” the value of θ⊤x into the range 0..1 so that we may interpret our classes as a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + numpy.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression\n",
    "Softmax Regression (also called multinomial logistic regression) is a generalized form of logistic regression. \n",
    "![](./images/Softmax_Classifier.png)\n",
    "\n",
    "\n",
    "In Softmax Regression (SMR), we simply replace the sigmoid logistic function by the so-called softmax function $\\phi_{softmax}(\\cdot)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e = numpy.exp(x - numpy.max(x))  # prevent overflow\n",
    "    if e.ndim == 1:\n",
    "        return e / numpy.sum(e, axis=0)\n",
    "    else:  \n",
    "        return e / numpy.array([numpy.sum(e, axis=1)]).T  # ndim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Class\n",
    "\n",
    "#### Regularization Equation\n",
    "![](./images/regularization.png)\n",
    "#### Gradient Descent (Log-Likelihood)\n",
    "Formula for the gradient of the log-likelihood with respect to the kth weight is\n",
    "![](./images/gd_loglikelihood.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, label, n_in, n_out):\n",
    "        self.x = input\n",
    "        self.y = label\n",
    "        self.Weights = numpy.zeros((n_in, n_out))  # initialize W 0\n",
    "        self.biases = numpy.zeros(n_out)          # initialize bias 0\n",
    "\n",
    "        # self.params = [self.Weights, self.biases]\n",
    "\n",
    "    def train(self, lr=0.1, input=None, L2_regularization=0.00):\n",
    "        if input is not None:\n",
    "            self.x = input\n",
    "\n",
    "        # p_y_given_x = sigmoid(numpy.dot(self.x, self.Weights) + self.biases)\n",
    "        p_y_given_x = softmax(numpy.dot(self.x, self.Weights) + self.biases)\n",
    "        d_y = self.y - p_y_given_x\n",
    "        \n",
    "        self.Weights += lr * numpy.dot(self.x.T, d_y) - lr * L2_regularization * self.Weights\n",
    "        self.biases += lr * numpy.mean(d_y, axis=0)\n",
    "        \n",
    "        # cost = self.negative_log_likelihood()\n",
    "        # return cost\n",
    "\n",
    "    def negative_log_likelihood(self):\n",
    "        # sigmoid_activation = sigmoid(numpy.dot(self.x, self.Weights) + self.biases)\n",
    "        sigmoid_activation = softmax(numpy.dot(self.x, self.Weights) + self.biases)\n",
    "\n",
    "        cross_entropy = - numpy.mean(\n",
    "            numpy.sum(self.y * numpy.log(sigmoid_activation) +\n",
    "            (1 - self.y) * numpy.log(1 - sigmoid_activation),\n",
    "                      axis=1))\n",
    "\n",
    "        return cross_entropy\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        # return sigmoid(numpy.dot(x, self.Weights) + self.biases)\n",
    "        return softmax(numpy.dot(x, self.Weights) + self.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run\n",
    "Now we will do a testing run using randomly generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_lr(learning_rate=0.01, n_epochs=200):\n",
    "    # training data\n",
    "    x = numpy.array([[1,1,1,0,0,0],\n",
    "                     [1,0,1,0,0,0],\n",
    "                     [1,1,1,0,0,0],\n",
    "                     [0,0,1,1,1,0],\n",
    "                     [0,0,1,1,0,0],\n",
    "                     [0,0,1,1,1,0]])\n",
    "    y = numpy.array([[1, 0],\n",
    "                     [1, 0],\n",
    "                     [1, 0],\n",
    "                     [0, 1],\n",
    "                     [0, 1],\n",
    "                     [0, 1]])\n",
    "\n",
    "\n",
    "    # construct LogisticRegression\n",
    "    classifier = LogisticRegression(input=x, label=y, n_in=6, n_out=2)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(n_epochs):\n",
    "        classifier.train(lr=learning_rate)\n",
    "        cost = classifier.negative_log_likelihood()\n",
    "        print ( 'Training epoch %d, cost is ' % epoch, cost)\n",
    "        learning_rate *= 0.95\n",
    "\n",
    "\n",
    "    # test\n",
    "    x = numpy.array([1, 1, 0, 0, 0, 0])\n",
    "    print ( classifier.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, cost is  1.34345264825\n",
      "Training epoch 1, cost is  1.30455598779\n",
      "Training epoch 2, cost is  1.2691578322\n",
      "Training epoch 3, cost is  1.23687047036\n",
      "Training epoch 4, cost is  1.2073565859\n",
      "Training epoch 5, cost is  1.1803220394\n",
      "Training epoch 6, cost is  1.15550971744\n",
      "Training epoch 7, cost is  1.13269430085\n",
      "Training epoch 8, cost is  1.11167781874\n",
      "Training epoch 9, cost is  1.0922858705\n",
      "Training epoch 10, cost is  1.07436441371\n",
      "Training epoch 11, cost is  1.05777703022\n",
      "Training epoch 12, cost is  1.04240259613\n",
      "Training epoch 13, cost is  1.02813329242\n",
      "Training epoch 14, cost is  1.01487290331\n",
      "Training epoch 15, cost is  1.00253535772\n",
      "Training epoch 16, cost is  0.991043476319\n",
      "Training epoch 17, cost is  0.980327892779\n",
      "Training epoch 18, cost is  0.970326122837\n",
      "Training epoch 19, cost is  0.960981759043\n",
      "Training epoch 20, cost is  0.95224377257\n",
      "Training epoch 21, cost is  0.944065906409\n",
      "Training epoch 22, cost is  0.936406146759\n",
      "Training epoch 23, cost is  0.929226261457\n",
      "Training epoch 24, cost is  0.922491396007\n",
      "Training epoch 25, cost is  0.916169719228\n",
      "Training epoch 26, cost is  0.91023211172\n",
      "Training epoch 27, cost is  0.904651891371\n",
      "Training epoch 28, cost is  0.899404570983\n",
      "Training epoch 29, cost is  0.894467643797\n",
      "Training epoch 30, cost is  0.889820393324\n",
      "Training epoch 31, cost is  0.885443724363\n",
      "Training epoch 32, cost is  0.881320012569\n",
      "Training epoch 33, cost is  0.877432970262\n",
      "Training epoch 34, cost is  0.873767526489\n",
      "Training epoch 35, cost is  0.870309719647\n",
      "Training epoch 36, cost is  0.867046601148\n",
      "Training epoch 37, cost is  0.863966148865\n",
      "Training epoch 38, cost is  0.861057189208\n",
      "Training epoch 39, cost is  0.858309326862\n",
      "Training epoch 40, cost is  0.855712881328\n",
      "Training epoch 41, cost is  0.853258829513\n",
      "Training epoch 42, cost is  0.850938753709\n",
      "Training epoch 43, cost is  0.848744794382\n",
      "Training epoch 44, cost is  0.846669607273\n",
      "Training epoch 45, cost is  0.844706324338\n",
      "Training epoch 46, cost is  0.842848518154\n",
      "Training epoch 47, cost is  0.841090169424\n",
      "Training epoch 48, cost is  0.83942563728\n",
      "Training epoch 49, cost is  0.837849632094\n",
      "Training epoch 50, cost is  0.83635719057\n",
      "Training epoch 51, cost is  0.834943652885\n",
      "Training epoch 52, cost is  0.833604641684\n",
      "Training epoch 53, cost is  0.832336042762\n",
      "Training epoch 54, cost is  0.831133987275\n",
      "Training epoch 55, cost is  0.829994835334\n",
      "Training epoch 56, cost is  0.828915160869\n",
      "Training epoch 57, cost is  0.827891737636\n",
      "Training epoch 58, cost is  0.826921526282\n",
      "Training epoch 59, cost is  0.826001662362\n",
      "Training epoch 60, cost is  0.825129445239\n",
      "Training epoch 61, cost is  0.824302327777\n",
      "Training epoch 62, cost is  0.823517906784\n",
      "Training epoch 63, cost is  0.82277391412\n",
      "Training epoch 64, cost is  0.822068208427\n",
      "Training epoch 65, cost is  0.82139876743\n",
      "Training epoch 66, cost is  0.82076368077\n",
      "Training epoch 67, cost is  0.820161143308\n",
      "Training epoch 68, cost is  0.819589448889\n",
      "Training epoch 69, cost is  0.81904698451\n",
      "Training epoch 70, cost is  0.818532224876\n",
      "Training epoch 71, cost is  0.818043727299\n",
      "Training epoch 72, cost is  0.817580126937\n",
      "Training epoch 73, cost is  0.817140132328\n",
      "Training epoch 74, cost is  0.816722521204\n",
      "Training epoch 75, cost is  0.816326136572\n",
      "Training epoch 76, cost is  0.815949883033\n",
      "Training epoch 77, cost is  0.815592723331\n",
      "Training epoch 78, cost is  0.815253675108\n",
      "Training epoch 79, cost is  0.814931807857\n",
      "Training epoch 80, cost is  0.81462624006\n",
      "Training epoch 81, cost is  0.814336136491\n",
      "Training epoch 82, cost is  0.814060705683\n",
      "Training epoch 83, cost is  0.813799197542\n",
      "Training epoch 84, cost is  0.8135509011\n",
      "Training epoch 85, cost is  0.813315142399\n",
      "Training epoch 86, cost is  0.813091282492\n",
      "Training epoch 87, cost is  0.812878715569\n",
      "Training epoch 88, cost is  0.812676867178\n",
      "Training epoch 89, cost is  0.812485192554\n",
      "Training epoch 90, cost is  0.812303175036\n",
      "Training epoch 91, cost is  0.812130324581\n",
      "Training epoch 92, cost is  0.811966176355\n",
      "Training epoch 93, cost is  0.811810289399\n",
      "Training epoch 94, cost is  0.811662245377\n",
      "Training epoch 95, cost is  0.811521647388\n",
      "Training epoch 96, cost is  0.81138811884\n",
      "Training epoch 97, cost is  0.811261302393\n",
      "Training epoch 98, cost is  0.811140858952\n",
      "Training epoch 99, cost is  0.811026466718\n",
      "Training epoch 100, cost is  0.810917820293\n",
      "Training epoch 101, cost is  0.810814629824\n",
      "Training epoch 102, cost is  0.810716620202\n",
      "Training epoch 103, cost is  0.810623530302\n",
      "Training epoch 104, cost is  0.810535112256\n",
      "Training epoch 105, cost is  0.810451130776\n",
      "Training epoch 106, cost is  0.810371362502\n",
      "Training epoch 107, cost is  0.810295595394\n",
      "Training epoch 108, cost is  0.810223628147\n",
      "Training epoch 109, cost is  0.810155269644\n",
      "Training epoch 110, cost is  0.810090338435\n",
      "Training epoch 111, cost is  0.810028662239\n",
      "Training epoch 112, cost is  0.80997007748\n",
      "Training epoch 113, cost is  0.809914428842\n",
      "Training epoch 114, cost is  0.809861568847\n",
      "Training epoch 115, cost is  0.809811357456\n",
      "Training epoch 116, cost is  0.809763661691\n",
      "Training epoch 117, cost is  0.809718355279\n",
      "Training epoch 118, cost is  0.809675318304\n",
      "Training epoch 119, cost is  0.809634436895\n",
      "Training epoch 120, cost is  0.80959560291\n",
      "Training epoch 121, cost is  0.809558713649\n",
      "Training epoch 122, cost is  0.809523671583\n",
      "Training epoch 123, cost is  0.809490384084\n",
      "Training epoch 124, cost is  0.809458763184\n",
      "Training epoch 125, cost is  0.809428725336\n",
      "Training epoch 126, cost is  0.809400191191\n",
      "Training epoch 127, cost is  0.809373085387\n",
      "Training epoch 128, cost is  0.809347336349\n",
      "Training epoch 129, cost is  0.809322876093\n",
      "Training epoch 130, cost is  0.809299640052\n",
      "Training epoch 131, cost is  0.809277566896\n",
      "Training epoch 132, cost is  0.809256598376\n",
      "Training epoch 133, cost is  0.809236679165\n",
      "Training epoch 134, cost is  0.809217756711\n",
      "Training epoch 135, cost is  0.809199781099\n",
      "Training epoch 136, cost is  0.809182704916\n",
      "Training epoch 137, cost is  0.809166483128\n",
      "Training epoch 138, cost is  0.809151072957\n",
      "Training epoch 139, cost is  0.809136433772\n",
      "Training epoch 140, cost is  0.809122526977\n",
      "Training epoch 141, cost is  0.809109315909\n",
      "Training epoch 142, cost is  0.809096765746\n",
      "Training epoch 143, cost is  0.809084843407\n",
      "Training epoch 144, cost is  0.80907351747\n",
      "Training epoch 145, cost is  0.809062758088\n",
      "Training epoch 146, cost is  0.809052536908\n",
      "Training epoch 147, cost is  0.809042826996\n",
      "Training epoch 148, cost is  0.809033602769\n",
      "Training epoch 149, cost is  0.809024839925\n",
      "Training epoch 150, cost is  0.809016515376\n",
      "Training epoch 151, cost is  0.809008607195\n",
      "Training epoch 152, cost is  0.809001094548\n",
      "Training epoch 153, cost is  0.808993957647\n",
      "Training epoch 154, cost is  0.808987177693\n",
      "Training epoch 155, cost is  0.80898073683\n",
      "Training epoch 156, cost is  0.808974618092\n",
      "Training epoch 157, cost is  0.808968805367\n",
      "Training epoch 158, cost is  0.808963283346\n",
      "Training epoch 159, cost is  0.808958037487\n",
      "Training epoch 160, cost is  0.808953053977\n",
      "Training epoch 161, cost is  0.808948319692\n",
      "Training epoch 162, cost is  0.808943822166\n",
      "Training epoch 163, cost is  0.808939549557\n",
      "Training epoch 164, cost is  0.808935490615\n",
      "Training epoch 165, cost is  0.808931634654\n",
      "Training epoch 166, cost is  0.80892797152\n",
      "Training epoch 167, cost is  0.80892449157\n",
      "Training epoch 168, cost is  0.808921185642\n",
      "Training epoch 169, cost is  0.808918045032\n",
      "Training epoch 170, cost is  0.808915061472\n",
      "Training epoch 171, cost is  0.808912227109\n",
      "Training epoch 172, cost is  0.808909534479\n",
      "Training epoch 173, cost is  0.808906976496\n",
      "Training epoch 174, cost is  0.808904546425\n",
      "Training epoch 175, cost is  0.808902237869\n",
      "Training epoch 176, cost is  0.808900044752\n",
      "Training epoch 177, cost is  0.808897961301\n",
      "Training epoch 178, cost is  0.80889598203\n",
      "Training epoch 179, cost is  0.808894101731\n",
      "Training epoch 180, cost is  0.808892315455\n",
      "Training epoch 181, cost is  0.808890618498\n",
      "Training epoch 182, cost is  0.808889006395\n",
      "Training epoch 183, cost is  0.808887474902\n",
      "Training epoch 184, cost is  0.808886019989\n",
      "Training epoch 185, cost is  0.808884637826\n",
      "Training epoch 186, cost is  0.808883324774\n",
      "Training epoch 187, cost is  0.808882077379\n",
      "Training epoch 188, cost is  0.808880892357\n",
      "Training epoch 189, cost is  0.808879766588\n",
      "Training epoch 190, cost is  0.808878697111\n",
      "Training epoch 191, cost is  0.808877681109\n",
      "Training epoch 192, cost is  0.80887671591\n",
      "Training epoch 193, cost is  0.808875798973\n",
      "Training epoch 194, cost is  0.808874927884\n",
      "Training epoch 195, cost is  0.808874100351\n",
      "Training epoch 196, cost is  0.808873314196\n",
      "Training epoch 197, cost is  0.80887256735\n",
      "Training epoch 198, cost is  0.808871857848\n",
      "Training epoch 199, cost is  0.808871183822\n",
      "[ 0.69203174  0.30796826]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
