{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "We will be implementing a 2 layer neural network in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#two layer neural network\n",
    "n_hidden=10\n",
    "n_input=10\n",
    "n_out=10\n",
    "#sample data\n",
    "n_sample=300\n",
    "#hyperparameters\n",
    "learning_rate=0.01\n",
    "momentum=0.9\n",
    "#non deterministic seeding\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#activation function\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Our very own Neural Network\n",
    "Update Rules\n",
    "![](./images/backprop.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x,t,v,w,bv,bw):\n",
    "    #x=input data\n",
    "    #t=output\n",
    "    #v,w= weights of the two layers\n",
    "    #bv and bw bias\n",
    "    \n",
    "    #forward pass- matrix mul+bias followed by a non linearity\n",
    "    A=np.dot(x,v)+bv\n",
    "    z=np.tanh(A)\n",
    "    \n",
    "    B=np.dot(z,w)+bw\n",
    "    y=sigmoid(B)\n",
    "    \n",
    "    #backprop\n",
    "    Ew=y-t\n",
    "    Ev=tanh_prime(A)*np.dot(w,Ew)\n",
    "    \n",
    "    #predict loss\n",
    "    #out[i, j] = a[i] * b[j]\n",
    "    dW=np.outer(z,Ew)\n",
    "    dV=np.outer(x,Ev)\n",
    "    \n",
    "    #cross entropy loss\n",
    "    loss=-np.mean( t* np.log(y)+(1-t)*np.log(1-y))\n",
    "    \n",
    "    #returning loss and deltas \n",
    "    return loss,(dV,dW,Ev,Ew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x,v,w,bv,bw):\n",
    "    A=np.dot(x,v)+bv\n",
    "    z=np.tanh(A)\n",
    "    \n",
    "    B=np.dot(z,w)+bw\n",
    "    return (sigmoid(B)>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating layers\n",
    "v=np.random.normal(scale=0.1,size=(n_input,n_hidden))\n",
    "w=np.random.normal(scale=0.1,size=(n_hidden,n_out))\n",
    "\n",
    "bv=np.zeros(n_hidden)\n",
    "bw=np.zeros(n_out)\n",
    "\n",
    "#array of values\n",
    "params=[v,w,bv,bw]\n",
    "\n",
    "#generating data\n",
    "x=np.random.binomial(1,0.5,(n_sample,n_input))\n",
    "t=x^1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:0.45465070, Time: 0.0312 s\n",
      "Epoch: 1, Loss:0.13697961, Time: 0.0212 s\n",
      "Epoch: 2, Loss:0.06206941, Time: 0.0263 s\n",
      "Epoch: 3, Loss:0.04092746, Time: 0.0168 s\n",
      "Epoch: 4, Loss:0.03159958, Time: 0.0287 s\n",
      "Epoch: 5, Loss:0.02592744, Time: 0.0233 s\n",
      "Epoch: 6, Loss:0.02199575, Time: 0.0196 s\n",
      "Epoch: 7, Loss:0.01907812, Time: 0.0173 s\n",
      "Epoch: 8, Loss:0.01682099, Time: 0.0307 s\n",
      "Epoch: 9, Loss:0.01502363, Time: 0.0226 s\n",
      "Epoch: 10, Loss:0.01356039, Time: 0.0187 s\n",
      "Epoch: 11, Loss:0.01234775, Time: 0.0304 s\n",
      "Epoch: 12, Loss:0.01132776, Time: 0.0282 s\n",
      "Epoch: 13, Loss:0.01045887, Time: 0.0209 s\n",
      "Epoch: 14, Loss:0.00971052, Time: 0.0214 s\n",
      "Epoch: 15, Loss:0.00905971, Time: 0.0217 s\n",
      "Epoch: 16, Loss:0.00848887, Time: 0.0303 s\n",
      "Epoch: 17, Loss:0.00798436, Time: 0.0202 s\n",
      "Epoch: 18, Loss:0.00753542, Time: 0.0189 s\n",
      "Epoch: 19, Loss:0.00713347, Time: 0.0257 s\n",
      "Epoch: 20, Loss:0.00677160, Time: 0.0224 s\n",
      "Epoch: 21, Loss:0.00644415, Time: 0.0318 s\n",
      "Epoch: 22, Loss:0.00614650, Time: 0.0265 s\n",
      "Epoch: 23, Loss:0.00587477, Time: 0.0301 s\n",
      "Epoch: 24, Loss:0.00562576, Time: 0.0197 s\n",
      "Epoch: 25, Loss:0.00539674, Time: 0.0270 s\n",
      "Epoch: 26, Loss:0.00518541, Time: 0.0173 s\n",
      "Epoch: 27, Loss:0.00498981, Time: 0.0207 s\n",
      "Epoch: 28, Loss:0.00480824, Time: 0.0294 s\n",
      "Epoch: 29, Loss:0.00463926, Time: 0.0187 s\n",
      "Epoch: 30, Loss:0.00448161, Time: 0.0310 s\n",
      "Epoch: 31, Loss:0.00433419, Time: 0.0206 s\n",
      "Epoch: 32, Loss:0.00419603, Time: 0.0180 s\n",
      "Epoch: 33, Loss:0.00406629, Time: 0.0219 s\n",
      "Epoch: 34, Loss:0.00394423, Time: 0.0287 s\n",
      "Epoch: 35, Loss:0.00382919, Time: 0.0232 s\n",
      "Epoch: 36, Loss:0.00372058, Time: 0.0244 s\n",
      "Epoch: 37, Loss:0.00361787, Time: 0.0201 s\n",
      "Epoch: 38, Loss:0.00352061, Time: 0.0192 s\n",
      "Epoch: 39, Loss:0.00342836, Time: 0.0257 s\n",
      "Epoch: 40, Loss:0.00334076, Time: 0.0290 s\n",
      "Epoch: 41, Loss:0.00325746, Time: 0.0210 s\n",
      "Epoch: 42, Loss:0.00317815, Time: 0.0181 s\n",
      "Epoch: 43, Loss:0.00310255, Time: 0.0206 s\n",
      "Epoch: 44, Loss:0.00303042, Time: 0.0183 s\n",
      "Epoch: 45, Loss:0.00296151, Time: 0.0291 s\n",
      "Epoch: 46, Loss:0.00289562, Time: 0.0315 s\n",
      "Epoch: 47, Loss:0.00283255, Time: 0.0178 s\n",
      "Epoch: 48, Loss:0.00277213, Time: 0.0248 s\n",
      "Epoch: 49, Loss:0.00271419, Time: 0.0235 s\n",
      "Epoch: 50, Loss:0.00265858, Time: 0.0226 s\n",
      "Epoch: 51, Loss:0.00260517, Time: 0.0186 s\n",
      "Epoch: 52, Loss:0.00255383, Time: 0.0296 s\n",
      "Epoch: 53, Loss:0.00250444, Time: 0.0234 s\n",
      "Epoch: 54, Loss:0.00245689, Time: 0.0184 s\n",
      "Epoch: 55, Loss:0.00241108, Time: 0.0253 s\n",
      "Epoch: 56, Loss:0.00236692, Time: 0.0338 s\n",
      "Epoch: 57, Loss:0.00232432, Time: 0.0275 s\n",
      "Epoch: 58, Loss:0.00228320, Time: 0.0274 s\n",
      "Epoch: 59, Loss:0.00224348, Time: 0.0275 s\n",
      "Epoch: 60, Loss:0.00220510, Time: 0.0255 s\n",
      "Epoch: 61, Loss:0.00216798, Time: 0.0245 s\n",
      "Epoch: 62, Loss:0.00213207, Time: 0.0282 s\n",
      "Epoch: 63, Loss:0.00209730, Time: 0.0279 s\n",
      "Epoch: 64, Loss:0.00206363, Time: 0.0341 s\n",
      "Epoch: 65, Loss:0.00203101, Time: 0.0216 s\n",
      "Epoch: 66, Loss:0.00199938, Time: 0.0242 s\n",
      "Epoch: 67, Loss:0.00196870, Time: 0.0213 s\n",
      "Epoch: 68, Loss:0.00193892, Time: 0.0285 s\n",
      "Epoch: 69, Loss:0.00191002, Time: 0.0317 s\n",
      "Epoch: 70, Loss:0.00188195, Time: 0.0273 s\n",
      "Epoch: 71, Loss:0.00185467, Time: 0.0228 s\n",
      "Epoch: 72, Loss:0.00182816, Time: 0.0235 s\n",
      "Epoch: 73, Loss:0.00180238, Time: 0.0291 s\n",
      "Epoch: 74, Loss:0.00177730, Time: 0.0267 s\n",
      "Epoch: 75, Loss:0.00175290, Time: 0.0200 s\n",
      "Epoch: 76, Loss:0.00172914, Time: 0.0238 s\n",
      "Epoch: 77, Loss:0.00170600, Time: 0.0211 s\n",
      "Epoch: 78, Loss:0.00168346, Time: 0.0302 s\n",
      "Epoch: 79, Loss:0.00166149, Time: 0.0173 s\n",
      "Epoch: 80, Loss:0.00164008, Time: 0.0216 s\n",
      "Epoch: 81, Loss:0.00161920, Time: 0.0292 s\n",
      "Epoch: 82, Loss:0.00159883, Time: 0.0201 s\n",
      "Epoch: 83, Loss:0.00157896, Time: 0.0304 s\n",
      "Epoch: 84, Loss:0.00155957, Time: 0.0229 s\n",
      "Epoch: 85, Loss:0.00154063, Time: 0.0255 s\n",
      "Epoch: 86, Loss:0.00152214, Time: 0.0276 s\n",
      "Epoch: 87, Loss:0.00150407, Time: 0.0241 s\n",
      "Epoch: 88, Loss:0.00148642, Time: 0.0243 s\n",
      "Epoch: 89, Loss:0.00146917, Time: 0.0265 s\n",
      "Epoch: 90, Loss:0.00145230, Time: 0.0220 s\n",
      "Epoch: 91, Loss:0.00143581, Time: 0.0214 s\n",
      "Epoch: 92, Loss:0.00141968, Time: 0.0308 s\n",
      "Epoch: 93, Loss:0.00140390, Time: 0.0323 s\n",
      "Epoch: 94, Loss:0.00138846, Time: 0.0290 s\n",
      "Epoch: 95, Loss:0.00137334, Time: 0.0208 s\n",
      "Epoch: 96, Loss:0.00135854, Time: 0.0280 s\n",
      "Epoch: 97, Loss:0.00134405, Time: 0.0228 s\n",
      "Epoch: 98, Loss:0.00132986, Time: 0.0309 s\n",
      "Epoch: 99, Loss:0.00131596, Time: 0.0220 s\n"
     ]
    }
   ],
   "source": [
    "totalloss=[]\n",
    "for epoch in range(100):\n",
    "    err=[]\n",
    "    upd=[0]*len(params)\n",
    "    t0=time.clock()\n",
    "    #for each datapoint update weights\n",
    "    for i in range(x.shape[0]):\n",
    "        loss,grad=train(x[i],t[i],*params)\n",
    "        #update loss\n",
    "        for a in range(len(params)):\n",
    "            params[a]-=upd[a]\n",
    "        for b in range(len(params)):\n",
    "            upd[b]=learning_rate*grad[b]+momentum*upd[b]\n",
    "            \n",
    "        err.append(loss)\n",
    "        totalloss.append(loss)\n",
    "        #print(loss)\n",
    "        \n",
    "    print('Epoch: %d, Loss:%.8f, Time: %.4f s' %(\n",
    "        epoch,np.mean(err),time.clock()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(totalloss)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data\n",
      "[1 1 1 0 0 0 0 1 0 1]\n",
      "Output should be\n",
      "[0 0 0 1 1 1 1 0 1 0]\n",
      "Output is\n",
      "[0 0 0 1 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "x=np.random.binomial(1,0.5,n_input)\n",
    "print('Input data');\n",
    "print(x)\n",
    "print('Output should be')\n",
    "print(x^1)\n",
    "print('Output is')\n",
    "print(predict(x,*params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
